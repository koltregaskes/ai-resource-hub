/**
 * Seed the glossary table with AI terms and definitions.
 */
import { getDB } from '../src/db/schema';

const terms = [
  {
    id: 'llm',
    term: 'Large Language Model (LLM)',
    definition: 'A type of artificial intelligence model trained on massive amounts of text data that can understand and generate human-like text. LLMs use transformer architecture and are the technology behind ChatGPT, Claude, Gemini, and similar tools.',
    plain_english: 'A computer program that has read billions of pages of text and learned to write and understand language like a human. When you chat with ChatGPT or Claude, you are talking to an LLM.',
    category: 'core',
    related_terms: 'transformer,neural-network,foundation-model',
    see_also: 'gpt,token,fine-tuning',
  },
  {
    id: 'token',
    term: 'Token',
    definition: 'The basic unit of text that language models process. A token is roughly 3-4 characters or about 75% of a word in English. Models read and generate text as sequences of tokens, and pricing is typically based on the number of tokens processed.',
    plain_english: 'Think of tokens as the "syllables" that AI reads. The word "hamburger" might be split into "ham", "bur", "ger" — three tokens. AI companies charge based on how many tokens you use, like paying per word.',
    category: 'core',
    related_terms: 'context-window,tokenisation',
    see_also: 'input-tokens,output-tokens',
  },
  {
    id: 'context-window',
    term: 'Context Window',
    definition: 'The maximum amount of text (measured in tokens) that a language model can consider at once. This includes both the input prompt and the generated output. Larger context windows allow the model to process longer documents and maintain longer conversations.',
    plain_english: 'The AI\'s short-term memory. A bigger context window means the AI can read longer documents or remember more of your conversation. If you paste a 100-page document, you need a model with a large enough context window to read it all.',
    category: 'core',
    related_terms: 'token,input-tokens,output-tokens',
    see_also: 'rag',
  },
  {
    id: 'prompt',
    term: 'Prompt',
    definition: 'The text input given to an AI model to generate a response. Prompts can range from simple questions to complex instructions with examples, context, and constraints. The quality of the prompt significantly affects the quality of the output.',
    plain_english: 'The message or instruction you type to the AI. Just like how you phrase a question to a person matters, how you phrase your prompt to an AI matters. A clearer prompt gets a better answer.',
    category: 'core',
    related_terms: 'prompt-engineering,system-prompt,few-shot',
    see_also: 'chain-of-thought,zero-shot',
  },
  {
    id: 'prompt-engineering',
    term: 'Prompt Engineering',
    definition: 'The practice of crafting effective prompts to get the best possible outputs from AI models. This includes techniques like providing examples, specifying format, assigning roles, and breaking complex tasks into steps.',
    plain_english: 'The art of talking to AI effectively. Like learning to ask the right questions, prompt engineering is about figuring out how to phrase your requests so the AI gives you exactly what you want.',
    category: 'techniques',
    related_terms: 'prompt,few-shot,chain-of-thought,system-prompt',
    see_also: 'zero-shot,role-prompting',
  },
  {
    id: 'hallucination',
    term: 'Hallucination',
    definition: 'When an AI model generates information that sounds plausible but is factually incorrect, made up, or not supported by its training data. Hallucinations are a known limitation of current language models and a key reason to verify AI-generated content.',
    plain_english: 'When the AI confidently makes something up. It might invent fake statistics, cite books that don\'t exist, or give you wrong dates — all while sounding completely sure of itself. Always double-check important facts.',
    category: 'concepts',
    related_terms: 'grounding,rag',
    see_also: 'temperature,guardrails',
  },
  {
    id: 'fine-tuning',
    term: 'Fine-tuning',
    definition: 'The process of further training a pre-trained model on a specific dataset to specialise it for a particular task or domain. Fine-tuning adjusts the model\'s weights to improve performance on specific types of inputs.',
    plain_english: 'Teaching an already-smart AI to be an expert in your specific area. Like how a general doctor can specialise in cardiology through additional training, fine-tuning makes a general AI model better at one particular job.',
    category: 'technical',
    related_terms: 'pre-training,transfer-learning,lora',
    see_also: 'rlhf,training-data',
  },
  {
    id: 'transformer',
    term: 'Transformer',
    definition: 'The neural network architecture that powers modern language models. Introduced in the 2017 paper "Attention Is All You Need," transformers use a mechanism called self-attention to process relationships between all parts of the input simultaneously.',
    plain_english: 'The breakthrough invention that made modern AI possible. Before transformers, AI read text word by word. Transformers can look at all the words at once and understand how they relate to each other — like reading a whole page instead of one word at a time.',
    category: 'technical',
    related_terms: 'attention,neural-network,self-attention',
    see_also: 'llm,foundation-model',
  },
  {
    id: 'temperature',
    term: 'Temperature',
    definition: 'A parameter that controls the randomness of an AI model\'s outputs. Lower temperatures (e.g., 0.1) produce more deterministic, focused responses. Higher temperatures (e.g., 1.0) produce more creative, varied, and sometimes unpredictable outputs.',
    plain_english: 'A dial that controls how creative or predictable the AI is. Turn it down for factual answers (like maths or coding). Turn it up for creative writing or brainstorming. Think of it like the difference between a careful accountant (low) and a jazz musician (high).',
    category: 'parameters',
    related_terms: 'top-p,sampling',
    see_also: 'inference,api',
  },
  {
    id: 'api',
    term: 'API (Application Programming Interface)',
    definition: 'A way for software applications to communicate with AI models programmatically. AI APIs allow developers to send prompts and receive responses in their own applications, websites, or tools without building the AI model themselves.',
    plain_english: 'The "behind the scenes" way to use AI. Instead of chatting in a browser window, developers use APIs to plug AI into their own apps — like how a food delivery app uses a maps API to show you where your driver is.',
    category: 'core',
    related_terms: 'endpoint,sdk,rate-limit',
    see_also: 'token,pricing',
  },
  {
    id: 'gpt',
    term: 'GPT (Generative Pre-trained Transformer)',
    definition: 'A family of language models developed by OpenAI. GPT models are trained to predict the next token in a sequence and can perform a wide range of language tasks. GPT-4, GPT-4o, and GPT-5 are prominent members of this family.',
    plain_english: 'The name for OpenAI\'s AI models — the ones behind ChatGPT. The letters stand for "Generative Pre-trained Transformer." Each new version (GPT-4, GPT-5) is smarter and more capable than the last.',
    category: 'models',
    related_terms: 'llm,openai,chatgpt',
    see_also: 'claude,gemini',
  },
  {
    id: 'rag',
    term: 'RAG (Retrieval-Augmented Generation)',
    definition: 'A technique that enhances AI responses by first retrieving relevant information from an external knowledge base, then using that information as context for generating answers. RAG helps reduce hallucinations and keeps responses grounded in specific data.',
    plain_english: 'Giving the AI a reference book before asking it questions. Instead of relying on memory alone, RAG lets the AI look up real information first — like how a student performs better on an open-book exam than a closed-book one.',
    category: 'techniques',
    related_terms: 'embedding,vector-database,grounding',
    see_also: 'context-window,hallucination',
  },
  {
    id: 'embedding',
    term: 'Embedding',
    definition: 'A numerical representation of text (or images, audio, etc.) in a high-dimensional vector space. Embeddings capture semantic meaning, so similar concepts have similar numeric representations. They are essential for search, recommendation, and RAG systems.',
    plain_english: 'Turning words into numbers that capture their meaning. The numbers for "dog" and "puppy" would be very similar, while "dog" and "algebra" would be very different. This is how AI understands that related things are related.',
    category: 'technical',
    related_terms: 'vector-database,similarity-search',
    see_also: 'rag,semantic-search',
  },
  {
    id: 'open-source',
    term: 'Open Source (in AI)',
    definition: 'AI models whose weights, architecture, and often training details are publicly available for anyone to use, modify, and deploy. Examples include Meta\'s Llama, Mistral, and DeepSeek models. Open-source models can be run locally or self-hosted.',
    plain_english: 'Free AI models that anyone can download and use. Like how Linux is a free operating system anyone can use, open-source AI models like Llama let you run AI on your own computer without paying a subscription.',
    category: 'concepts',
    related_terms: 'open-weights,llama,mistral',
    see_also: 'self-hosting,inference',
  },
  {
    id: 'inference',
    term: 'Inference',
    definition: 'The process of running a trained AI model to generate predictions or outputs from new inputs. When you send a prompt to ChatGPT and receive a response, that\'s inference. Inference speed and cost are key factors in AI deployment.',
    plain_english: 'Actually using the AI to get answers. Training is like studying for an exam; inference is like taking the exam. Every time you ask ChatGPT a question, it\'s doing inference — and that costs computing power.',
    category: 'technical',
    related_terms: 'latency,throughput,gpu',
    see_also: 'token,speed,api',
  },
  {
    id: 'chain-of-thought',
    term: 'Chain-of-Thought (CoT)',
    definition: 'A prompting technique where the AI is encouraged to show its reasoning step by step before giving a final answer. This significantly improves accuracy on complex tasks like maths, logic, and multi-step reasoning problems.',
    plain_english: 'Asking the AI to "show its working" like a maths teacher would. When the AI thinks through problems step by step instead of jumping to an answer, it makes fewer mistakes — especially with tricky questions.',
    category: 'techniques',
    related_terms: 'prompt-engineering,reasoning,few-shot',
    see_also: 'prompt,zero-shot',
  },
  {
    id: 'few-shot',
    term: 'Few-Shot Learning',
    definition: 'A prompting technique where you provide a small number of examples (typically 2-5) in your prompt to show the model the pattern or format you want. The model learns from these examples to produce similar outputs.',
    plain_english: 'Teaching the AI by showing examples. Instead of explaining what you want, you show it: "Here\'s example input 1 and its ideal output. Here\'s example 2 and its output. Now do the same for this new input."',
    category: 'techniques',
    related_terms: 'zero-shot,one-shot,in-context-learning',
    see_also: 'prompt-engineering,prompt',
  },
  {
    id: 'zero-shot',
    term: 'Zero-Shot Learning',
    definition: 'When a model performs a task without any examples in the prompt — relying entirely on its pre-trained knowledge and the task description alone. Modern LLMs are remarkably capable zero-shot learners.',
    plain_english: 'Asking the AI to do something without showing it any examples first. Like asking someone to write a haiku who already knows what a haiku is — you don\'t need to show them examples, you just ask.',
    category: 'techniques',
    related_terms: 'few-shot,in-context-learning',
    see_also: 'prompt-engineering,prompt',
  },
  {
    id: 'system-prompt',
    term: 'System Prompt',
    definition: 'A special instruction given to an AI model that sets its behaviour, personality, constraints, and role for the entire conversation. System prompts are typically hidden from the end user and are set by the application developer.',
    plain_english: 'The "secret instructions" that tell the AI how to behave. When you use a customer service chatbot, it has a system prompt saying something like "You are a helpful customer service agent for Company X. Only answer questions about our products."',
    category: 'core',
    related_terms: 'prompt,role-prompting,guardrails',
    see_also: 'prompt-engineering,temperature',
  },
  {
    id: 'benchmark',
    term: 'Benchmark',
    definition: 'A standardised test used to evaluate and compare AI model performance. Common benchmarks include MMLU (general knowledge), GPQA (graduate-level science), HumanEval (coding), and MATH (mathematical reasoning). Benchmarks enable objective comparison between models.',
    plain_english: 'A standardised exam for AI models. Just like students take SATs to compare performance, AI models take benchmarks. A model scoring 90% on MMLU means it answered 90% of a massive multiple-choice knowledge test correctly.',
    category: 'concepts',
    related_terms: 'mmlu,gpqa,humaneval,swe-bench',
    see_also: 'quality-score,leaderboard',
  },
  {
    id: 'multimodal',
    term: 'Multimodal',
    definition: 'AI models that can process and generate multiple types of data — typically text, images, audio, and video. GPT-4o, Gemini, and Claude 4 are multimodal models that can understand images and generate text about them.',
    plain_english: 'An AI that can see, hear, and read — not just one of those. A multimodal AI can look at a photo of your broken dishwasher, listen to the weird noise it makes, and tell you what\'s wrong with it, all in one conversation.',
    category: 'concepts',
    related_terms: 'vision,audio,text-to-image',
    see_also: 'modality,llm',
  },
  {
    id: 'rlhf',
    term: 'RLHF (Reinforcement Learning from Human Feedback)',
    definition: 'A training technique where human evaluators rank model outputs, and this feedback is used to train a reward model that guides the AI to produce more helpful, harmless, and honest responses. RLHF is a key technique behind ChatGPT and Claude.',
    plain_english: 'Training the AI by having humans rate its answers as good or bad. The AI learns to give more answers like the ones humans liked and fewer answers like the ones they didn\'t. It\'s like training a dog with treats — rewarding good behaviour.',
    category: 'technical',
    related_terms: 'reinforcement-learning,reward-model,constitutional-ai',
    see_also: 'fine-tuning,alignment',
  },
  {
    id: 'alignment',
    term: 'AI Alignment',
    definition: 'The challenge of ensuring that AI systems behave in accordance with human values, intentions, and ethical principles. Alignment research aims to make AI systems that are helpful, harmless, and honest.',
    plain_english: 'Making sure AI does what humans actually want, not just what it was technically told to do. Like the story of the genie who grants wishes too literally — alignment is about making sure the AI understands the spirit of your request, not just the letter.',
    category: 'concepts',
    related_terms: 'rlhf,constitutional-ai,safety',
    see_also: 'guardrails,red-teaming',
  },
  {
    id: 'agent',
    term: 'AI Agent',
    definition: 'An AI system that can autonomously perform multi-step tasks by planning, using tools, and making decisions. Unlike simple chatbots, agents can browse the web, write code, manage files, and take actions in the real world based on high-level instructions.',
    plain_english: 'An AI that can actually do things, not just talk. Instead of just answering "here\'s how to book a flight," an AI agent can actually search flights, compare prices, and book one for you — handling multiple steps on its own.',
    category: 'concepts',
    related_terms: 'tool-use,function-calling,autonomy',
    see_also: 'chain-of-thought,planning',
  },
  {
    id: 'neural-network',
    term: 'Neural Network',
    definition: 'A computing system inspired by the biological neural networks in the human brain. Neural networks consist of layers of interconnected nodes (neurons) that process information and learn patterns from data. They are the foundation of modern AI.',
    plain_english: 'A computer program loosely inspired by how the brain works. It has layers of simple processing units connected together, and it learns by adjusting these connections based on examples — like how practising a skill strengthens pathways in your brain.',
    category: 'technical',
    related_terms: 'deep-learning,layer,weights',
    see_also: 'transformer,training-data',
  },
  {
    id: 'gpu',
    term: 'GPU (Graphics Processing Unit)',
    definition: 'Originally designed for rendering graphics, GPUs are now the primary hardware for training and running AI models. Their ability to perform many calculations in parallel makes them essential for the matrix operations that power neural networks.',
    plain_english: 'The powerful computer chips that AI runs on. Originally made for video games, GPUs turned out to be perfect for AI because they can do thousands of calculations at the same time. NVIDIA makes the most popular ones (like the H100 and B200).',
    category: 'infrastructure',
    related_terms: 'nvidia,tpu,compute',
    see_also: 'inference,training',
  },
  {
    id: 'vector-database',
    term: 'Vector Database',
    definition: 'A specialised database designed to store and search high-dimensional vectors (embeddings). Vector databases enable semantic search — finding content by meaning rather than exact keyword matches. Popular examples include Pinecone, Weaviate, and ChromaDB.',
    plain_english: 'A special database that understands meaning. Instead of searching for the exact word "car," a vector database can find documents about "automobile," "vehicle," or "driving" because it understands they\'re related concepts.',
    category: 'infrastructure',
    related_terms: 'embedding,similarity-search,semantic-search',
    see_also: 'rag,pinecone',
  },
  {
    id: 'tokenisation',
    term: 'Tokenisation',
    definition: 'The process of breaking text into tokens before feeding it to a language model. Different models use different tokenisation schemes — some split by word parts (subword tokenisation), while others use byte-pair encoding (BPE).',
    plain_english: 'How AI chops up your text into pieces it can understand. Like breaking a sentence into individual building blocks. The word "unhappiness" might become "un", "happiness" — two tokens. Different AI models chop text up differently.',
    category: 'technical',
    related_terms: 'token,bpe,vocabulary',
    see_also: 'context-window,input-tokens',
  },
  {
    id: 'diffusion-model',
    term: 'Diffusion Model',
    definition: 'A type of generative AI model used primarily for image and video generation. Diffusion models work by learning to gradually remove noise from random data until a clear image emerges. DALL-E 3, Stable Diffusion, and Midjourney use this approach.',
    plain_english: 'The technology behind AI image generators. Imagine starting with TV static and slowly "cleaning it up" until a picture appears. Diffusion models learn to do this cleanup process, guided by your text description of what the image should look like.',
    category: 'technical',
    related_terms: 'text-to-image,stable-diffusion,midjourney',
    see_also: 'generative-ai,dall-e',
  },
  {
    id: 'generative-ai',
    term: 'Generative AI',
    definition: 'AI systems that can create new content — text, images, music, code, video — rather than just analysing or classifying existing content. The current generative AI boom began with ChatGPT\'s launch in November 2022.',
    plain_english: 'AI that creates new things. Unlike older AI that just sorted emails into spam/not-spam, generative AI writes essays, creates artwork, composes music, and writes code. If it can create something new, it\'s generative AI.',
    category: 'core',
    related_terms: 'llm,diffusion-model,text-to-image',
    see_also: 'chatgpt,foundation-model',
  },
  {
    id: 'foundation-model',
    term: 'Foundation Model',
    definition: 'A large AI model trained on broad data that can be adapted to a wide range of downstream tasks. Foundation models (like GPT-4, Claude, or Llama) serve as a "foundation" that can be fine-tuned or prompted for specific applications.',
    plain_english: 'A general-purpose AI brain that can be specialised for different jobs. Like how a university degree gives you broad knowledge that you can then apply to a specific career, a foundation model provides general intelligence that can be tailored to specific tasks.',
    category: 'core',
    related_terms: 'pre-training,fine-tuning,transfer-learning',
    see_also: 'llm,transformer',
  },
  {
    id: 'latency',
    term: 'Latency',
    definition: 'The time delay between sending a request to an AI model and receiving the first token of the response. Lower latency means faster-feeling interactions. Latency depends on model size, server load, and network distance.',
    plain_english: 'How long you wait before the AI starts answering. When you press Send and there\'s a pause before text appears — that\'s latency. Smaller, faster models have lower latency. Bigger, smarter models often have higher latency.',
    category: 'parameters',
    related_terms: 'speed,throughput,ttft',
    see_also: 'inference,api',
  },
  {
    id: 'top-p',
    term: 'Top-p (Nucleus Sampling)',
    definition: 'A parameter that controls output diversity by limiting token selection to the smallest set of tokens whose cumulative probability exceeds p. Lower values (e.g., 0.1) make output more focused; higher values (e.g., 0.95) allow more variety.',
    plain_english: 'Another way to control how creative or predictable the AI is, alongside temperature. At low top-p, the AI only picks from the most likely next words. At high top-p, it considers more unusual word choices too.',
    category: 'parameters',
    related_terms: 'temperature,sampling,top-k',
    see_also: 'inference,api',
  },
  {
    id: 'guardrails',
    term: 'Guardrails',
    definition: 'Safety mechanisms built into AI systems to prevent harmful, biased, or inappropriate outputs. Guardrails include content filters, topic restrictions, output validation, and behaviour guidelines enforced through system prompts and fine-tuning.',
    plain_english: 'Safety bumpers for AI. Like guardrails on a road that stop you going off a cliff, AI guardrails stop the model from generating harmful content, sharing dangerous information, or behaving in ways its creators didn\'t intend.',
    category: 'concepts',
    related_terms: 'alignment,safety,content-filter',
    see_also: 'rlhf,system-prompt',
  },
  {
    id: 'lora',
    term: 'LoRA (Low-Rank Adaptation)',
    definition: 'An efficient fine-tuning technique that trains only a small number of additional parameters instead of modifying the entire model. LoRA dramatically reduces the compute and memory required for fine-tuning, making it practical on consumer hardware.',
    plain_english: 'A clever shortcut for customising AI models without needing a supercomputer. Instead of retraining the whole model (expensive), LoRA adds small "adapter" layers that steer the model\'s behaviour — like adding a lens filter to a camera instead of buying a new camera.',
    category: 'technical',
    related_terms: 'fine-tuning,adapter,qlora',
    see_also: 'open-source,training',
  },
  {
    id: 'attention',
    term: 'Attention Mechanism',
    definition: 'A technique in neural networks that allows the model to focus on the most relevant parts of the input when generating each part of the output. Self-attention, the key innovation in transformers, lets every token attend to every other token.',
    plain_english: 'How AI decides what to pay attention to. When reading "The cat sat on the mat because it was tired," the attention mechanism helps the AI understand that "it" refers to "the cat" — by paying attention to the relationship between those words.',
    category: 'technical',
    related_terms: 'transformer,self-attention,multi-head-attention',
    see_also: 'context-window,neural-network',
  },
  {
    id: 'chatgpt',
    term: 'ChatGPT',
    definition: 'A conversational AI product by OpenAI that provides a chat interface to GPT models. Launched in November 2022, ChatGPT sparked the current AI boom and remains one of the most widely used AI tools. Available in free and paid tiers.',
    plain_english: 'The AI chatbot that started it all — made by OpenAI. You type questions or instructions, and it writes back. Think of it as a very knowledgeable assistant you can have a conversation with. The free version uses a good model; the paid version ($20/month) gets you the best models.',
    category: 'products',
    related_terms: 'openai,gpt,gpt-4o',
    see_also: 'claude,gemini',
  },
  {
    id: 'claude',
    term: 'Claude',
    definition: 'A family of AI assistants developed by Anthropic. Known for strong reasoning, long context windows, careful safety alignment, and nuanced instruction following. Current models include Claude Opus, Sonnet, and Haiku in the Claude 4.x family.',
    plain_english: 'Anthropic\'s AI assistant — a major competitor to ChatGPT. Claude is known for being thoughtful, following complex instructions well, and having a very large "memory" (context window). Available through claude.ai or the API.',
    category: 'products',
    related_terms: 'anthropic,opus,sonnet,haiku',
    see_also: 'chatgpt,gemini',
  },
  {
    id: 'gemini',
    term: 'Gemini',
    definition: 'Google\'s family of multimodal AI models, available through the Gemini app, Google AI Studio, and Vertex AI. The Gemini family includes Ultra, Pro, Flash, and Nano tiers optimised for different use cases and price points.',
    plain_english: 'Google\'s answer to ChatGPT. Gemini is built into Google products (Search, Gmail, Docs) and is available as a standalone chatbot. It comes in different sizes — from the tiny Nano (runs on phones) to the powerful Ultra (for complex tasks).',
    category: 'products',
    related_terms: 'google,deepmind,bard',
    see_also: 'chatgpt,claude',
  },
  {
    id: 'input-tokens',
    term: 'Input Tokens',
    definition: 'The tokens in the prompt or request sent to an AI model. Input tokens include the system prompt, conversation history, and the user\'s current message. AI APIs typically charge a separate (usually lower) rate for input tokens.',
    plain_english: 'The words you send to the AI. When you type a question, those words get converted into tokens, and you\'re charged for each one. Input tokens are usually cheaper than output tokens — it costs less for the AI to read than to write.',
    category: 'core',
    related_terms: 'output-tokens,token,pricing',
    see_also: 'context-window,api',
  },
  {
    id: 'output-tokens',
    term: 'Output Tokens',
    definition: 'The tokens generated by an AI model in its response. Output tokens are typically more expensive than input tokens because generation requires more computation. Most APIs let you set a maximum output token limit.',
    plain_english: 'The words the AI writes back to you. These are usually more expensive than input tokens because generating new text takes more computing power than reading text. Think of it like how writing an essay takes more effort than reading one.',
    category: 'core',
    related_terms: 'input-tokens,token,pricing',
    see_also: 'max-output,speed',
  },
  {
    id: 'self-hosting',
    term: 'Self-Hosting',
    definition: 'Running an AI model on your own hardware or cloud infrastructure rather than using a third-party API. Self-hosting gives full control over data privacy, latency, and costs but requires technical expertise and significant compute resources.',
    plain_english: 'Running AI on your own computer or server instead of using someone else\'s service. Like cooking at home vs. ordering takeaway — you have more control and potentially lower costs, but you need the equipment and skills to do it.',
    category: 'infrastructure',
    related_terms: 'open-source,gpu,ollama',
    see_also: 'inference,api',
  },
  {
    id: 'reasoning-model',
    term: 'Reasoning Model',
    definition: 'AI models specifically designed to break down complex problems into steps and reason through them before answering. Examples include OpenAI\'s o3/o4 series, DeepSeek R1, and Gemini Deep Think. They trade speed for accuracy on hard problems.',
    plain_english: 'AI models that "think harder" before answering. Instead of blurting out the first answer, reasoning models take extra time to work through problems step by step — like how you\'d solve a hard maths problem on paper rather than in your head.',
    category: 'concepts',
    related_terms: 'chain-of-thought,o3,deepseek-r1',
    see_also: 'llm,benchmark',
  },
  {
    id: 'grounding',
    term: 'Grounding',
    definition: 'Techniques that connect AI model outputs to verifiable sources of information, reducing hallucinations. Grounding can involve RAG, web search integration, or connecting models to specific databases or knowledge bases.',
    plain_english: 'Keeping AI honest by connecting it to real facts. Instead of letting the AI just make things up from memory, grounding forces it to check actual sources — like making a student cite their sources in an essay.',
    category: 'techniques',
    related_terms: 'rag,hallucination,citations',
    see_also: 'vector-database,search',
  },
  {
    id: 'function-calling',
    term: 'Function Calling (Tool Use)',
    definition: 'The ability of AI models to generate structured calls to external tools, APIs, or functions. Instead of just generating text, the model outputs a specific function name and parameters, which the application then executes and returns results for.',
    plain_english: 'When AI can use tools, not just talk. Function calling lets AI actually do things — check the weather, search a database, send an email — by generating specific "commands" that your software then carries out.',
    category: 'technical',
    related_terms: 'agent,tool-use,api',
    see_also: 'json,structured-output',
  },
  {
    id: 'model-collapse',
    term: 'Model Collapse',
    definition: 'A phenomenon where AI models trained on AI-generated data progressively lose quality and diversity over generations. As more AI-generated content fills the internet, this becomes a growing concern for training future models.',
    plain_english: 'What happens when AI trains on other AI\'s homework. Like a game of telephone where the message gets worse each time, if AI models keep learning from each other\'s outputs instead of real human-written text, the quality gradually degrades.',
    category: 'concepts',
    related_terms: 'training-data,synthetic-data',
    see_also: 'pre-training,data-quality',
  },
];

const db = getDB();

const stmt = db.prepare(`
  INSERT OR REPLACE INTO glossary (id, term, definition, plain_english, category, related_terms, see_also)
  VALUES (?, ?, ?, ?, ?, ?, ?)
`);

const insertAll = db.transaction(() => {
  for (const t of terms) {
    stmt.run(t.id, t.term, t.definition, t.plain_english, t.category, t.related_terms, t.see_also);
  }
});

insertAll();
console.log(`Seeded ${terms.length} glossary terms.`);
